{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ab73efa-d0a2-4fed-b3a9-728418abcd98",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5326773-760c-4966-8213-1c29734cafb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from matplotlib import cm as CM\n",
    "from scipy import io\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.neighbors import KDTree\n",
    "from torchinfo import summary\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9fe40a5-5901-47a5-8af3-d546bd1bde94",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive = None\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03db484",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./\"\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "PATH = PATH if drive is None else \"/content/drive/MyDrive/self-learn/crowd-counting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5cdcb5f-dbbe-40f6-9499-fa1372a2712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(PATH)\n",
    "\n",
    "from constants import *\n",
    "from models import CSRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "875141f1-226d-4474-be8b-14ec98d9464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = root if drive is None else \"/content/drive/MyDrive/crowd-counting\"\n",
    "path = path  #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16edb0fd-b34e-4389-931d-17e44235384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"CSRNet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2626a181",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c02acba-1f77-4f98-9fa4-5f106cb569a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CSRNet                                   [8, 1, 224, 224]          --\n",
       "├─Sequential: 1-1                        [8, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-1                       [8, 64, 224, 224]         1,792\n",
       "│    └─ReLU: 2-2                         [8, 64, 224, 224]         --\n",
       "│    └─Conv2d: 2-3                       [8, 64, 224, 224]         36,928\n",
       "│    └─ReLU: 2-4                         [8, 64, 224, 224]         --\n",
       "│    └─MaxPool2d: 2-5                    [8, 64, 112, 112]         --\n",
       "│    └─Conv2d: 2-6                       [8, 128, 112, 112]        73,856\n",
       "│    └─ReLU: 2-7                         [8, 128, 112, 112]        --\n",
       "│    └─Conv2d: 2-8                       [8, 128, 112, 112]        147,584\n",
       "│    └─ReLU: 2-9                         [8, 128, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-10                   [8, 128, 56, 56]          --\n",
       "│    └─Conv2d: 2-11                      [8, 256, 56, 56]          295,168\n",
       "│    └─ReLU: 2-12                        [8, 256, 56, 56]          --\n",
       "│    └─Conv2d: 2-13                      [8, 256, 56, 56]          590,080\n",
       "│    └─ReLU: 2-14                        [8, 256, 56, 56]          --\n",
       "│    └─Conv2d: 2-15                      [8, 256, 56, 56]          590,080\n",
       "│    └─ReLU: 2-16                        [8, 256, 56, 56]          --\n",
       "│    └─MaxPool2d: 2-17                   [8, 256, 28, 28]          --\n",
       "│    └─Conv2d: 2-18                      [8, 512, 28, 28]          1,180,160\n",
       "│    └─ReLU: 2-19                        [8, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-20                      [8, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-21                        [8, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-22                      [8, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-23                        [8, 512, 28, 28]          --\n",
       "├─Sequential: 1-2                        [8, 64, 28, 28]           --\n",
       "│    └─Conv2d: 2-24                      [8, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-25                        [8, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-26                      [8, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-27                        [8, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-28                      [8, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-29                        [8, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-30                      [8, 256, 28, 28]          1,179,904\n",
       "│    └─ReLU: 2-31                        [8, 256, 28, 28]          --\n",
       "│    └─Conv2d: 2-32                      [8, 128, 28, 28]          295,040\n",
       "│    └─ReLU: 2-33                        [8, 128, 28, 28]          --\n",
       "│    └─Conv2d: 2-34                      [8, 64, 28, 28]           73,792\n",
       "│    └─ReLU: 2-35                        [8, 64, 28, 28]           --\n",
       "├─Conv2d: 1-3                            [8, 1, 28, 28]            65\n",
       "==========================================================================================\n",
       "Total params: 16,263,489\n",
       "Trainable params: 16,263,489\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 165.90\n",
       "==========================================================================================\n",
       "Input size (MB): 4.82\n",
       "Forward/backward pass size (MB): 947.37\n",
       "Params size (MB): 65.05\n",
       "Estimated Total Size (MB): 1017.24\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CSRNet()\n",
    "summary(model, (8, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acd5d80",
   "metadata": {},
   "source": [
    "# Dataset preprocessing and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bcb88998-06b5-4815-b091-bc2cace44c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveResize:\n",
    "    def __init__(self, out_shape):\n",
    "        self.out_shape = out_shape\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = v2.Resize(self.out_shape, antialias=True)(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "49f511af-f751-460c-9850-ca3b552e38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmap_transform = v2.Compose(\n",
    "    [\n",
    "        lambda x: np.expand_dims(x, -1),  ## add channel dim to dmap\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(\n",
    "            torch.float32, scale=True\n",
    "        ),  ## these two are equivalent to the deprecated v2.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "img_transform = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.ColorJitter(),\n",
    "        v2.RandomGrayscale(p=0.1),\n",
    "        lambda x: v2.functional.adjust_gamma(\n",
    "            x, gamma=torch.FloatTensor(1).uniform_(0.8, 1.2)\n",
    "        ),\n",
    "        # lambda x: x/255\n",
    "    ]\n",
    ")\n",
    "\n",
    "### warning: if multiprocessing w/ num_workers > 0, lambda functions may break and custom classes may be needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "188bf8f5-a449-4d9e-bfa9-0d09e7248728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrowdDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    CrowdDataset for ShanghaiTech_A.\n",
    "    Indexing returns a tuple (img, gt_dmap), where `img` is a 3D tensor (C=3, W, H),\n",
    "    and gt_dmap is the ground truth density map (1, W//ds_scale, H//ds_scale).\n",
    "    ds_scale (int=4): Downsampling scale factor. In the case of MCNN, the model predicts a density map of size (W//4, H//4), so the ground\n",
    "    truth map must be appropriately resized, and for CSRNet, the scale is 8.\n",
    "    interpolate (bool=False): If True (for CSRNet), the model attempts to interpolate its downscaled output map back to the original\n",
    "    image dimensions. i.e. (540 x 256) would become (540//ds_scale * ds_scale x 256//ds_scale * ds_scale) = (536 x 256)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, path=\"ShanghaiTech_A\", ds_scale=4, interpolate=False):\n",
    "\n",
    "        assert split in [\"train\", \"test\"], \"`split` must be either `train` or `test`.\"\n",
    "        self.data_path = f\"{path}/{split}_data\"\n",
    "        self.ds_scale = ds_scale\n",
    "        self.interpolate = interpolate\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        try:\n",
    "            gt_dmap = np.load(f\"{self.data_path}/gt_maps/GT_IMG_{index+1}.npy\")\n",
    "            img = plt.imread(\n",
    "                f\"{self.data_path}/images/IMG_{index+1}.jpg\"\n",
    "            ).copy()  # copy for writability\n",
    "            if len(img.shape) == 2:\n",
    "                img = np.repeat(\n",
    "                    img[:, :, np.newaxis], 3, axis=-1\n",
    "                )  ## convert B&W to RGB via repeat\n",
    "        except FileNotFoundError as e:\n",
    "            raise Exception(\"File not found. Index may be out of bounds.\") from e\n",
    "\n",
    "        if self.interpolate:\n",
    "            out_shape = (\n",
    "                img.shape[0] // self.ds_scale * self.ds_scale,\n",
    "                img.shape[1] // self.ds_scale * self.ds_scale,\n",
    "            )\n",
    "        else:\n",
    "            out_shape = (img.shape[0] // self.ds_scale, img.shape[1] // self.ds_scale)\n",
    "\n",
    "        gt_dmap = dmap_transform(gt_dmap)\n",
    "        gt_dmap = AdaptiveResize(out_shape)(gt_dmap)\n",
    "        if not self.interpolate:\n",
    "            gt_dmap *= self.ds_scale ** 2  ## re-normalize values after downsampling\n",
    "        img = img_transform(img)\n",
    "\n",
    "        return img, gt_dmap\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(f\"{self.data_path}/images\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0350244a-31b2-4397-8bad-4ec41e907415",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CrowdDataset(split=\"train\", ds_scale=8, interpolate=True)\n",
    "test_data = CrowdDataset(split=\"test\", ds_scale=8, interpolate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ab45b1-bf14-4c8e-bf6f-2cf75486d728",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d43b9-465c-4350-a63c-be977eebcc18",
   "metadata": {},
   "source": [
    "#### Note we use a batch size of 1, because PyTorch Dataloaders require identically sized images.\n",
    "To circumvent this, there are a number of methods, such as writing a custom collate function. We will perform **gradient accumulation** over `BATCH_SIZE` steps in our training function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cc016eef-1085-4384-ac79-7a24c2e12cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0115b6c5-c66b-4bd6-b8e8-45824d6f2ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for img, gt in train_loader:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d72c0a0-477e-4436-8634-af706d7b82fc",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "197d3444-042f-479a-8197-1121b22d33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, criterion, device):\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    train_losses, val_losses = [], []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "        for step, (img, gt) in enumerate(train_loader):\n",
    "\n",
    "            img, gt = img.to(device), gt.to(device)\n",
    "\n",
    "            out = model(img)\n",
    "            loss = criterion(out, gt)\n",
    "            loss /= BATCH_SIZE\n",
    "            train_losses.append(loss.item())  # every step\n",
    "            loss.backward()\n",
    "\n",
    "            ########### NOTE: fix later, consider when last batch is not of size BATCH_SIZE!\n",
    "            if (step + 1) % BATCH_SIZE == 0 or (step + 1) == len(train_loader):\n",
    "\n",
    "                # # monitor overall gradient norm\n",
    "                # grads = [\n",
    "                #     param.grad.detach().flatten()\n",
    "                #     for param in model.parameters()\n",
    "                #     if param.grad is not None\n",
    "                # ]\n",
    "                # norm = torch.cat(grads).norm()\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if (step + 1) % (BATCH_SIZE * PRINT_ITERS) == 0 and step != 0:\n",
    "\n",
    "                mae = abs(out.sum() - gt.sum())\n",
    "                print(\n",
    "                    f\"Step: {step+1}/{len(train_loader)} | Loss: {loss.item():.2e} |\",\n",
    "                    f\"Pred: {out.sum():.3f} | True: {gt.sum():.3f} |\",\n",
    "                    f\"MAE: {mae:.3f}\",\n",
    "                )\n",
    "\n",
    "                # val_loss, val_acc = eval(model, val_loader, criterion, device)\n",
    "                # val_losses.append(val_loss)\n",
    "                # val_accuracies.append(val_acc)\n",
    "                # print(\n",
    "                #     f\"Step: {step}/{len(train_loader)}, Running Average Loss: {np.mean(train_losses):.3f} |\",\n",
    "                #     f\"Val Loss: {val_loss:.3f} | Val Acc: {val_acc:.3f} | Grad Norm: {norm:.2f}\",\n",
    "                # )\n",
    "                # model.train()\n",
    "\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            },\n",
    "            f\"{path}/checkpoints/{MODEL_NAME}_EPOCH_{epoch+1}_SEED_{SEED}.pt\",\n",
    "        )\n",
    "\n",
    "        with open(\n",
    "            f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_train_losses.json\", \"w\"\n",
    "        ) as f:\n",
    "            json.dump(train_losses, f)\n",
    "\n",
    "        # with open(\n",
    "        #     f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_val_losses.json\", \"w\"\n",
    "        # ) as f2:\n",
    "        #     json.dump(val_losses, f2)\n",
    "\n",
    "        # with open(\n",
    "        #     f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_val_accuracies.json\", \"w\"\n",
    "        # ) as f3:\n",
    "        #     json.dump(val_accuracies, f3)\n",
    "\n",
    "        # torch.save(deltas, f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_deltas.pt\")\n",
    "\n",
    "    # return train_losses, val_losses, val_accuracies\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7abb88b7-3067-45e8-bdb0-189fa59633d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CSRNet()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f5e73-88fa-48c9-902f-c445d4f7fe83",
   "metadata": {},
   "source": [
    "## Driver code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "64d3e54d-b170-4590-9493-07d9253a0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2  ### for now. move to 32/64 etc.\n",
    "PRINT_ITERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b016eb0b-9a94-46c5-b229-d9be3112ee04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Step: 2/300 | Loss: 8.32e-06 | Pred: 32.190 | True: 332.012 | MAE: 299.822\n",
      "Step: 4/300 | Loss: 8.99e-02 | Pred: 215902.000 | True: 1157.785 | MAE: 214744.219\n"
     ]
    }
   ],
   "source": [
    "## Driver code\n",
    "train_losses = train(model, train_loader, test_loader, optimizer, criterion, device)\n",
    "# train_losses, val_losses, val_accuracies = train(\n",
    "#     model, train_loader, test_loader, optimizer, criterion, device\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf15ebb-17cb-4d55-96ab-3889bbde0029",
   "metadata": {},
   "source": [
    "## todo: eval function on val and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03466301-d2b4-45b9-8eaf-78efb075ff67",
   "metadata": {},
   "source": [
    "#### temporary informal inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "bffbe3a8-4948-4a3a-8426-2b4f4326f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 64\n",
    "gt_dmap = np.load(f\"{data_path}/test_data/gt_maps/GT_IMG_{test_idx+1}.npy\")\n",
    "img = plt.imread(f\"{data_path}/test_data/images/IMG_{test_idx+1}.jpg\").copy()\n",
    "if len(img.shape) == 2:\n",
    "    img = np.repeat(img[:, :, np.newaxis], 3, axis=-1)\n",
    "out_shape = (img.shape[0] // 4, img.shape[1] // 4)\n",
    "\n",
    "gt_dmap = dmap_transform(gt_dmap)\n",
    "gt_dmap = AdaptiveResize(out_shape)(gt_dmap)\n",
    "gt_dmap *= 4 ** 2  ## re-normalize values after resizing\n",
    "img = img_transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "5b0766b4-6c7a-4fdf-bbe1-898b748a760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(img.unsqueeze(0)).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a01f1f-70dc-49e3-b03a-022931a7d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(16, 12))\n",
    "ax[0].imshow(img.cpu().permute(1, 2, 0))\n",
    "ax[0].set_title(\"Original Image\")\n",
    "ax[1].imshow(gt_dmap.cpu().permute(1, 2, 0))\n",
    "ax[1].set_title(f\"Ground Truth DM: {gt_dmap.sum():.3f}\")\n",
    "ax[2].imshow(out.cpu().permute(1, 2, 0))\n",
    "ax[2].set_title(f\"Pred DM: {out.sum():.3f}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
